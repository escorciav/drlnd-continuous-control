{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: https://user-images.githubusercontent.com/10624937/43851024-320ba930-9aff-11e8-8493-ee547c6af349.gif \"Trained Agent\"\n",
    "\n",
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will see a Policy Gradient approach in action learning to move a mechanical arm to reach a given target location. This was part of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "![Trained Agent][image1]\n",
    "\n",
    "## 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning stage\n",
    "\n",
    "Along the same line of though that we discussed in our [previous project](https://github.com/escorciav/drlnd-navigation/blob/master/Navigation.ipynb), we are interested on aiding our agent, a robotic arm, to learn through experiences to reach a target location. Last time, we revisited the famous DQN algorithm that learns to play Atari at a human performance level by itself. However, this algorithm is a bit limited for our _Reacher_ project. In this case our action space is continous, thus it's more natural to focus on the way that actions are taken.\n",
    "\n",
    "_Why would we do that?_\n",
    "\n",
    "In the case of DQN, we rely on finding the action that maximizes an action-value function. This maximization could be too difficult in a continuous space with infinite possible values. Moreover, there are cases in which it's simpler to come up with a way to react or reason about a plausible action instead of constructing a perfect action-value function from which we sample the actions of our agent.\n",
    "\n",
    "OK. Sounds good. How do we do it?\n",
    "\n",
    "In a nutshell, we will adjust a bit the architecture of our agent's brain to model the sensorimotor relationships explicitly. Wait, sensoriwhat? basically how to translate the perceptual inputs into actions. In the next sections, we will revisit the concept of policy gradients and how to adjust the DQN algorithm to incorporate the sensorimotor stream.\n",
    "\n",
    "### 3.1 Policy Gradients\n",
    "\n",
    "The term policy gradients correspond to a big familiy of algorithms that learn the sensorimotor relationships of any agent using gradient based methods.\n",
    "\n",
    "_What is a policy?_\n",
    "\n",
    "A policy is the strategy devised by our agent to take a given action from a given sensor/perceptual inputs. In other words, it is a shorter way to say sensorimotor relationships ðŸ˜„. We will denote our policy as $\\mu_\\theta$ which corresponds to a function that transforms the current state, $s_t$, of our agent into an action $a_t$ i.e. $a_t = \\mu(s_t|\\theta^\\mu)$ where $\\theta^\\mu$ corresponds to the parameters of our policy function. As you may imagine, $\\theta^\\mu$ will correspond to the parameters of a neural network.\n",
    "\n",
    "In other words, policiy-based methods model the relationship between the state space and action space explictly. This is simply a conceptual step introducing a new element into our reinforcement learning problem. \n",
    "\n",
    "_What about the gradients?_\n",
    "\n",
    "The gradient part comes from the fact that the parameters $\\theta^\\mu$ of the policy are found based on gradient evaluations of an optimization problem which we will described in the next section.\n",
    "    \n",
    "### 3.2 Learning\n",
    "\n",
    "Let's remind the reinforcement learning setup, at every moment in time our agent collects information about the environment, i.e. itself and the things in front of it (the so-called state space of 33 dimensions here); and it takes action (adjusting the torque of the 4 joints forming the so-called action space). Subsequently, it will recive another packet of information, i.e. a new 33 dimensional vector; and the reward (+0.1 when the hand is at the goal location). This process will repeat for a period of time, and our agent will be able to assess if a given policy $\\mu_theta$ allows it to keep the extreme of the hand at the desired location.\n",
    "      \n",
    "It seems that the setup has not really changed wrt the one of DQN algorithm, described [before](https://github.com/escorciav/drlnd-navigation/blob/master/Navigation.ipynb). At every time step, we could try to change the parameters, $\\theta$, of our \"Policy\" such that get rewards often. In layman terms, we could think about our agent as having a module (or brain), which we call the Actor, that tries to associate state with actions that are associated with high rewards.\n",
    "\n",
    "_How do we update the parameter of our Actor?_\n",
    "\n",
    "As we mentioned before, this setup is not unfamiliar. Thus, we could still resort on the notion of the TD-Error to define how to update our policy. Assuming a experience tuple (s, a, r', s'), the TD-Error is defined as:\n",
    "\n",
    "$L = r + \\gamma Q(s', a) - Q(s, a)$\n",
    "\n",
    "where $\\gamma$ is a discount factor, and $Q$ is a state-value function. The $Q$ function assesses the relevance of the current state-action. Thus, it can help us to shape our _Actor_ module such that it's encouraged to take certain actions for a given state configuration. To do so we need to enhance our agent with a new module that approximates the Q-function, this module is commonly called a _Critic_, and it judges the actions taken by our Actor module. In practice, the _Critic_ is simply a function $Q(s, a;\\theta^Q)$ which can be approximated by a neural network.\n",
    "\n",
    "With this setup, we have all the pieces of our lego ready. Our agent is composed by two modules an _Actor_ $a = \\mu(s|\\theta^\\mu)$ that translates the perceptual info on the environment into an action $s$ and a _Critic_ $Q(s, a|\\theta^Q)$ that judges if a given state-action pair will get high reward.\n",
    "\n",
    "The great news of keeping a similar formluation is that we can rely on previous successful ideas. In particular, the concept of (i) a _Replay Buffer_ to store experience and avoid learning correlated samples; and (ii) _soft-updates or target functions_ which avoids the drifting problem associated by updating our agent parameters with its own guess, estimates of Q, to fast.\n",
    "\n",
    "Finally, in this particular implementation we exploit the properties of parallel computing to make use of the experiences of multiple agents using the same _Actor-Critic_ parameters ($\\theta^\\mu,\\theta^Q$). This is a great strategy to increase the diversity of samples in our Replay Buffer.\n",
    "\n",
    "### 3.3 Actor-Critic details\n",
    "\n",
    "For simpliciy and modularity, all the inner details of our actor and critic networks are in this [module](https://github.com/escorciav/drlnd-continuous-control/blob/master/model.py). We used the same hyper-parameter configuration described in the [paper](https://arxiv.org/pdf/1509.02971.pdf).\n",
    "\n",
    "For the _Actor_, we use a MLP with a single hidden layer. In practice, we project the state space into a space of 400 dimension through a Linear layer. The size of our hidden layer is 300 dimensions. Finally, our output layer projects the output of our hidden layer into the size of the action space. With the exception of the last linear projection layer, all the others are followed by a ReLU non-linearity. The last linear layer is followed by a `tanh` non-linearity ensuring that the output range fits between $[-1, 1]$ i.e. the admisible values of our actuators.\n",
    "\n",
    "The _Critic_ network is similar to the _Actor_. The only notable differences are:\n",
    "\n",
    "- The size of our output layer is reduced to 1 and we remove any non-lineary after it, as it is estimating the value the combined state and action pair.\n",
    "    \n",
    "- We fuses the state and action in the \"hidden layer\" of our network as presented in the [DDPG paper](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "    \n",
    "The following figure depicts the details of our Actor adn Critic networks.\n",
    "\n",
    "![Architecture](https://github.com/escorciav/drlnd-continuous-control/raw/master/data/diagram.png)\n",
    "\n",
    "_Optimization and exploration and others details_\n",
    "\n",
    "We update the target networks after each learning steps with a $\\tau$ value equal to 1e-3 i.e. after each learning update our target actor-critic networks move closer to the new actor-critics networks by 0.1%. The size of our buffer is 100k experiences and the batch size is 128. The discount factor for future rewards is set 0.99.\n",
    "\n",
    "As epxloration strategy, we use noise in the action space. In particular, the noise is sampled from [Ornstein-Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) with $\\theta=0.15$, $\\sigma=0.2$. In this way, we can add temporally-correlated noise which is of particular interest for continous problems such us our Reacher.\n",
    "\n",
    "Here, we use the ADAM optimizer with different two learning rates 1e-4 and 1e-3, for the critic and the actor respectively. The target actor-critic networks are updated via soft-updates using $\\tau=1e-3$\n",
    "\n",
    "### 3.4 Put everything together\n",
    "\n",
    "Now it's time to train our own agent to solve the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Ž         | 25/1000 [08:33<5:27:00, 20.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [25]: score (averaged over agents) 4.4575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 50/1000 [16:57<5:16:02, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [50]: score (averaged over agents) 13.4995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 75/1000 [25:36<5:20:51, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [75]: score (averaged over agents) 21.4985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 100/1000 [34:19<5:19:46, 21.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [100]: score (averaged over agents) 34.1285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–Ž        | 125/1000 [42:50<4:57:43, 20.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [125]: score (averaged over agents) 29.9595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 150/1000 [51:21<4:50:07, 20.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [150]: score (averaged over agents) 33.9490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–‹        | 163/1000 [55:47<4:43:44, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge solved after 164 episodes\n"
     ]
    }
   ],
   "source": [
    "random_seed = 1701\n",
    "num_episodes = 1000\n",
    "is_solved_score = 30\n",
    "print_freq = 0.025\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size,\n",
    "              random_seed=random_seed)\n",
    "best_score = -1\n",
    "scores_window = deque(maxlen=100)\n",
    "scores_all = []\n",
    "if print_freq < 1:\n",
    "    print_freq = max(int(print_freq * num_episodes), 1)\n",
    "\n",
    "for i_episode in tqdm(range(1, num_episodes + 1)):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    \n",
    "    while True:\n",
    "        actions = agent.act(states)\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        agent.step(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if any(dones):\n",
    "            break\n",
    "    \n",
    "    avg_score = scores.mean()\n",
    "    scores_all.append(avg_score)\n",
    "    scores_window.append(avg_score)\n",
    "    \n",
    "    if i_episode % print_freq == 0:\n",
    "        print(f'Episode [{i_episode}]: score (averaged over agents) '\n",
    "              f'{avg_score:.4f}')\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        agent.save()\n",
    "        best_score = avg_score\n",
    "    \n",
    "    if np.mean(scores_window) >= is_solved_score:\n",
    "        print(f'Challenge solved after {i_episode:d} episodes')\n",
    "        agent.save()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the progression of our agent along the course of all the episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, len(scores_all) + 1), scores_all,\n",
    "         lw=3)\n",
    "plt.xlabel('Episodes', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Watch a smart agent\n",
    "\n",
    "It's time to watch the performance of our autonomous agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents): 35.93649919675663\n"
     ]
    }
   ],
   "source": [
    "random_seed = 1701\n",
    "agent = Agent(state_size=state_size, action_size=action_size,\n",
    "              random_seed=random_seed)\n",
    "agent.load('checkpoint.pth', map_location='cpu')\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = agent.act(states)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents): {}'.format(np.mean(scores)))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Departing words\n",
    "\n",
    "It was great to see how a simple algorithm such as DDPG managed to solved this particular continous control challenge. There are definitely more steps to improve the learning of our autonomous agents. For example,\n",
    "\n",
    "- Prioritized experience replay. Currently, the past experiences used to train the brain is by sampling uniform random. However, we could argue that some experiences may be more \"important\" than others. Moreover, it is possible that such \"important\" experiences are infrequent and our agents ends up sampling them less often. Therefore, it would be ideal to implement a mechanism to sampling experiences in a non-uniform way based on some criterion.\n",
    "\n",
    "- Noise in the parameteres space. To account for more exploration and possibly more robustness to unseen conditions and states, it would be important to implemente the noise in the parameters strategy dicussed [here](https://blog.openai.com/better-exploration-with-parameter-noise/).\n",
    "\n",
    "- Other kind of algorithms. As we mentioned before the term _Policy Gradient_ corresponds to a family of methods. Thus, there are other aglorithmic formulations of the learning problem such as A2C, PPO, etc. It would be interesting to see how the difference between each other in a common challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
